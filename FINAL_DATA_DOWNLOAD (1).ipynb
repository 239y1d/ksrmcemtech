{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"uFZEPK2SZNh8"},"outputs":[],"source":["# Robust, Resumable, Parallel PDF Scraper with Full Metadata Extraction and Logging\n","\n","import os\n","import requests\n","import pandas as pd\n","from bs4 import BeautifulSoup\n","from urllib.parse import urljoin\n","from concurrent.futures import ThreadPoolExecutor, as_completed\n","import time\n","import urllib3\n","from tqdm import tqdm\n","import random\n","import string\n","import shutil\n","\n","# Disable SSL warnings\n","urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n","\n","# Constants\n","BASE_URL = \"https://eparlib.nic.in\"\n","START_URL = f\"{BASE_URL}/handle/123456789/9\"\n","START_PAGE = 701\n","END_PAGE = 702\n","RUN_ID = ''.join(random.choices(string.ascii_uppercase + string.digits, k=5))\n","RUN_TAG = f\"p{START_PAGE}-{END_PAGE}_{RUN_ID}\"\n","OUTPUT_DIR = f\"scrape_output_{RUN_TAG}\"\n","PDF_DIR = os.path.join(OUTPUT_DIR, f\"pdfs_{RUN_TAG}\")\n","CSV_FILE = os.path.join(OUTPUT_DIR, f\"metadata_{RUN_TAG}.csv\")\n","XLSX_FILE = os.path.join(OUTPUT_DIR, f\"metadata_{RUN_TAG}.xlsx\")\n","LOG_FILE = os.path.join(OUTPUT_DIR, f\"errors_{RUN_TAG}.log\")\n","HEADERS = {'User-Agent': 'Mozilla/5.0'}\n","\n","# Setup\n","os.makedirs(PDF_DIR, exist_ok=True)\n","os.makedirs(OUTPUT_DIR, exist_ok=True)\n","if not os.path.exists(CSV_FILE):\n","    empty_df = pd.DataFrame(columns=[\n","        \"Page\", \"Row\", \"Date\", \"Title\", \"Type\", \"PDF File Name\",\n","        \"Language\", \"Members\", \"Session\", \"Ministry\",\n","        \"Starred Type\", \"Question No\", \"Member Name\", \"Session No\", \"Part No\",\n","        \"Full Type\", \"Author\", \"Error\"])\n","    empty_df.to_csv(CSV_FILE, index=False)\n","\n","# Load existing metadata and downloaded files\n","existing_df = pd.read_csv(CSV_FILE)\n","existing_titles = set(existing_df[\"Title\"].dropna().astype(str).str.strip())\n","downloaded_files = set(os.listdir(PDF_DIR))\n","\n","# Log function\n","def log_error(msg):\n","    with open(LOG_FILE, \"a\") as f:\n","        f.write(msg + \"\\n\")\n","\n","# Check if file is a valid PDF\n","def is_valid_pdf(file_path):\n","    try:\n","        with open(file_path, 'rb') as f:\n","            return f.read(5) == b\"%PDF-\"\n","    except Exception:\n","        return False\n","\n","# Extract field from metadata table\n","def extract_field(soup, label):\n","    try:\n","        row = soup.find(\"td\", class_=\"metadataFieldLabel\", string=lambda x: x and label in x)\n","        if row:\n","            return row.find_next(\"td\").get_text(strip=True)\n","    except:\n","        return \"\"\n","    return \"\"\n","\n","# Process a single row\n","def process_row(page_num, row_num, row):\n","    try:\n","        tds = row.find_all(\"td\")\n","        if len(tds) != 4:\n","            return None\n","\n","        date = tds[0].text.strip()\n","        title = tds[1].text.strip()\n","        type_ = tds[2].text.strip()\n","        view_link = urljoin(BASE_URL, tds[3].find(\"a\")[\"href\"])\n","\n","        if title in existing_titles:\n","            return None\n","\n","        res = requests.get(view_link, headers=HEADERS, verify=False, timeout=10)\n","        soup = BeautifulSoup(res.text, \"html.parser\")\n","\n","        file_name = \"\"\n","        for a in soup.select(\"a.btn.btn-primary\"):\n","            href = a.get(\"href\", \"\")\n","            if \"hindi\" in href.lower():\n","                continue\n","            file_url = urljoin(BASE_URL, href)\n","            file_name = os.path.basename(file_url)\n","            if file_name in downloaded_files:\n","                break\n","            r = requests.get(file_url, headers=HEADERS, verify=False, timeout=15)\n","            if r.headers.get(\"Content-Type\") != \"application/pdf\":\n","                raise ValueError(\"Not a PDF\")\n","            path = os.path.join(PDF_DIR, file_name)\n","            with open(path, \"wb\") as f:\n","                f.write(r.content)\n","            if not is_valid_pdf(path):\n","                raise ValueError(\"Corrupted or partial PDF\")\n","            downloaded_files.add(file_name)\n","            break\n","\n","        metadata = {\n","            \"Page\": page_num,\n","            \"Row\": row_num,\n","            \"Date\": date,\n","            \"Title\": title,\n","            \"Type\": type_,\n","            \"PDF File Name\": file_name,\n","            \"Language\": extract_field(soup, \"Language\"),\n","            \"Members\": extract_field(soup, \"Members\"),\n","            \"Session\": extract_field(soup, \"Session\"),\n","            \"Ministry\": extract_field(soup, \"Ministry\"),\n","            \"Starred Type\": extract_field(soup, \"Starred\"),\n","            \"Question No\": extract_field(soup, \"Question No\"),\n","            \"Member Name\": extract_field(soup, \"Member\"),\n","            \"Session No\": extract_field(soup, \"Session No\"),\n","            \"Part No\": extract_field(soup, \"Part No\"),\n","            \"Full Type\": extract_field(soup, \"Type\"),\n","            \"Author\": extract_field(soup, \"Author\"),\n","            \"Error\": \"\"\n","        }\n","\n","        pd.DataFrame([metadata]).to_csv(CSV_FILE, mode=\"a\", header=False, index=False)\n","        return metadata\n","\n","    except Exception as e:\n","        error_msg = f\"Page {page_num}, Row {row_num}: {str(e)}\"\n","        log_error(error_msg)\n","        metadata = {\n","            \"Page\": page_num,\n","            \"Row\": row_num,\n","            \"Date\": date if 'date' in locals() else \"\",\n","            \"Title\": title if 'title' in locals() else \"\",\n","            \"Type\": type_ if 'type_' in locals() else \"\",\n","            \"PDF File Name\": file_name if 'file_name' in locals() else \"\",\n","            \"Language\": \"\",\n","            \"Members\": \"\",\n","            \"Session\": \"\",\n","            \"Ministry\": \"\",\n","            \"Starred Type\": \"\",\n","            \"Question No\": \"\",\n","            \"Member Name\": \"\",\n","            \"Session No\": \"\",\n","            \"Part No\": \"\",\n","            \"Full Type\": \"\",\n","            \"Author\": \"\",\n","            \"Error\": str(e)\n","        }\n","        pd.DataFrame([metadata]).to_csv(CSV_FILE, mode=\"a\", header=False, index=False)\n","        return None\n","\n","# Main scraping loop\n","def scrape_pages(start_page, end_page):\n","    for page_num in range(start_page, end_page + 1):\n","        url = f\"{START_URL}?offset={page_num * 20}\"\n","        print(f\"\\nðŸ”„ Processing Page {page_num}: {url}\")\n","        try:\n","            res = requests.get(url, headers=HEADERS, verify=False, timeout=10)\n","            soup = BeautifulSoup(res.text, \"html.parser\")\n","            rows = soup.select(\"table.panel.table.table-bordered.table-hover tr\")[1:]\n","\n","            if not rows:\n","                print(\"âœ… No more rows on this page.\")\n","                break\n","\n","            with ThreadPoolExecutor(max_workers=5) as executor:\n","                entries = [(page_num, i, row) for i, row in enumerate(rows)]\n","                futures = [executor.submit(process_row, *entry) for entry in entries]\n","                for _ in tqdm(as_completed(futures), total=len(futures), desc=f\"Page {page_num}\"):\n","                    pass\n","\n","        except Exception as e:\n","            log_error(f\"Page {page_num} failed: {e}\")\n","\n","# Run scraper\n","scrape_pages(START_PAGE, END_PAGE)\n","\n","# Save Excel version\n","pd.read_csv(CSV_FILE).to_excel(XLSX_FILE, index=False)\n","\n","# Zip final folder\n","shutil.make_archive(OUTPUT_DIR, 'zip', OUTPUT_DIR)\n","\n","print(f\"\\nâœ… Scraping completed. All files saved in folder '{OUTPUT_DIR}' and zipped as '{OUTPUT_DIR}.zip'\")\n","from google.colab import files\n","files.download(f\"{OUTPUT_DIR}.zip\")"]}]}